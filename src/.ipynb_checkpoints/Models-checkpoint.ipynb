{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa746eb8-b454-41dd-8f45-fd9197935a1c",
   "metadata": {},
   "source": [
    "# Trabalho Prático - Sistema de recuperação textual \n",
    "\n",
    "#### SCC0282 - Recuperção de Informação \n",
    "\n",
    "##### Alunos:\n",
    "Pedro Afonso Fazio Michalichem - 10734196 <br> Rafael Silva - 7564023 <br> Ricardo Atakiama - 10262482 <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aab956-2887-4887-9d45-fa9f62e78c59",
   "metadata": {},
   "source": [
    "#### Para este trabalho foram utilizadas os seguintes pacotes da biblioteca nltk:\n",
    "- 'tokenize' para transformação do texto em tokens\n",
    "- 'corpus' para remoção de stop words, \n",
    "- 'stem' para steeming (radicalização)\n",
    "\n",
    "#### - xmltodict \n",
    "- para trabalhar com xml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "684c4ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ricar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ricar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import nltk\n",
    "import xmltodict\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eba6f8f-a740-49b7-8ed8-48340fca9c7e",
   "metadata": {},
   "source": [
    "Flags para controle dos modos de pesquisa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff02fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOULD_ENABLE_STOP_WORDS=True\n",
    "SHOULD_ENABLE_STEMMING=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e33e4af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    regex = re.compile('[^a-zA-Z0-9\\s]')\n",
    "    text_returned = re.sub(regex, '', text)\n",
    "    return text_returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03939acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_all_unique_words_and_freq(dict_global, words):\n",
    "    for word in set(words):\n",
    "        if word in dict_global.keys():\n",
    "            dict_global[word] += words.count(word)\n",
    "        else:\n",
    "            dict_global[word] = words.count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b7a4d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stops = set(stopwords.words('english'))\n",
    "    \n",
    "def remove_stop_words(all_words):\n",
    "    words = []\n",
    "    for word in all_words: \n",
    "        if word not in en_stops:\n",
    "            words.append(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ee2443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_words(words):\n",
    "    return [ps.stem(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b92fb2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_words_from_text(text):\n",
    "    text = remove_special_characters(text)\n",
    "    words = word_tokenize(text.lower())\n",
    "    \n",
    "    if SHOULD_ENABLE_STOP_WORDS:\n",
    "        words = remove_stop_words(words)\n",
    "    if SHOULD_ENABLE_STEMMING:\n",
    "        words = stem_words(words)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66fb80f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(words_index, global_dictionary, filename):\n",
    "    with open(filename) as fd:\n",
    "        doc = xmltodict.parse(fd.read())\n",
    "        text = doc['DOC']['TEXT']\n",
    "        index = os.path.basename(filename)\n",
    "        if isinstance(text, str):\n",
    "            words = prepare_words_from_text(text)\n",
    "            finding_all_unique_words_and_freq(global_dictionary, words)\n",
    "            words_index[index] = words\n",
    "        else:\n",
    "            print(index + \" is empty\")\n",
    "            words_index[index] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5763251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    return list(set(lst1) & set(lst2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3604718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, docId, freq):\n",
    "        self.freq = freq\n",
    "        self.doc = docId\n",
    "        self.next = None\n",
    "    \n",
    "    def __str__(self):        \n",
    "        return 'doc:' + str(self.doc) + ', freq:' + str(self.freq)\n",
    "\n",
    "class LinkedList:\n",
    "    def __init__(self):\n",
    "        self.head = None\n",
    "        self.tail = None\n",
    "        self.n_docs = 0\n",
    "    \n",
    "    def print_list(self):\n",
    "        aux = self.head\n",
    "        while aux:\n",
    "            print(aux)\n",
    "            aux = aux.next\n",
    "    \n",
    "    def get_doclist(self):\n",
    "        l = []\n",
    "        aux = self.head\n",
    "        while aux:\n",
    "            l.append([aux.doc, aux.freq])\n",
    "            aux = aux.next\n",
    "        return l\n",
    "    \n",
    "    def add_doc(self, doc, freq):\n",
    "        node = Node(doc, freq)        \n",
    "        if self.head == None:\n",
    "            self.head = node        \n",
    "        else:\n",
    "            self.tail.next = node\n",
    "        self.tail = node\n",
    "        self.n_docs += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992a26c3",
   "metadata": {},
   "source": [
    "# Terms Database\n",
    "\n",
    "A seção a seguir é responsável por construir o banco de dados por termos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9255154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../FIRE2010/en.doc.2010/TELEGRAPH_UTF8/2006_utf8/nation\n",
      "1060408_nation_story_6073156.utf8 is empty\n"
     ]
    }
   ],
   "source": [
    "# root = \"../FIRE2010/en.doc.2010/TELEGRAPH_UTF8\"\n",
    "root = \"../FIRE2010/en.doc.2010/TELEGRAPH_UTF8/2006_utf8/nation\"\n",
    "\n",
    "global_dictionary = {}\n",
    "words_index = {}\n",
    "processed_files = 0\n",
    "\n",
    "for path, subdirs, files in os.walk(root):\n",
    "    print(\"processing \" + path)\n",
    "    for name in files:\n",
    "        filename = os.path.join(path, name)\n",
    "        if filename.endswith(\".utf8\"):\n",
    "            processed_files += 1\n",
    "            process_file(words_index, global_dictionary, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7df0f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_id_list = [*words_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb9eb9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_all = sorted(set(global_dictionary.keys()))\n",
    "linked_list_data = {}\n",
    "\n",
    "for word in unique_words_all:\n",
    "    linked_list_data[word] = LinkedList()\n",
    "\n",
    "for doc in words_index.keys():\n",
    "    words = words_index[doc]\n",
    "    for word in set(words):\n",
    "        linked_list_data[word].add_doc(doc, words.count(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc3d3ce",
   "metadata": {},
   "source": [
    "# Queries\n",
    "\n",
    "A seção a seguir é responsável por carregar o arquivo com as queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76f59dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_filename = \"../FIRE2010/en.topics.76-125.2010.txt\"\n",
    "\n",
    "queries = {}\n",
    "\n",
    "with open(queries_filename) as fd:\n",
    "    doc = xmltodict.parse(fd.read())\n",
    "    for query in doc['topics']['top']: \n",
    "        queries[query['num']] = {\n",
    "            'title': query['title'],\n",
    "            'desc': query['desc'],\n",
    "            'narr': query['narr'],\n",
    "            '@lang': query['@lang']\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fcf127",
   "metadata": {},
   "source": [
    "# Probabilistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e71b8ff0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def probabilistic(query):\n",
    "    query_words = prepare_words_from_text(query)\n",
    "    answer = {}\n",
    "    for doc in words_index.keys():\n",
    "        words = words_index[doc]\n",
    "        common_words = intersection(query_words, words)\n",
    "        score = 0\n",
    "        for ki in common_words:\n",
    "            score += math.log10((len(docs_id_list)+0.5)/(linked_list_data[ki].n_docs+0.5))\n",
    "        answer[doc] = score\n",
    "    return answer\n",
    "\n",
    "\n",
    "res = probabilistic(queries['76']['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "181b83de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in res.keys():\n",
    "#     if int(res[key]) > 0:\n",
    "#         print(key, res[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccb853f",
   "metadata": {},
   "source": [
    "# Vector Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5b5965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.zeros((len(unique_words_all), len(docs_id_list)))\n",
    "\n",
    "for i in range(len(unique_words_all)):\n",
    "    word = unique_words_all[i]\n",
    "    postings = linked_list_data[word].get_doclist()\n",
    "    ni = linked_list_data[word].n_docs\n",
    "    for node in postings:\n",
    "        docID = docs_id_list.index(node[0])\n",
    "        freq = node[1]\n",
    "        m[i, docID] = freq\n",
    "#     print(word, m[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e44ecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.zeros((len(unique_words_all), len(docs_id_list)))\n",
    "for i in range(len(unique_words_all)):\n",
    "    word = unique_words_all[i]\n",
    "    postings = linked_list_data[word].get_doclist()\n",
    "    ni = linked_list_data[word].n_docs\n",
    "    idf = math.log2(len(docs_id_list)/ni)\n",
    "    for node in postings:\n",
    "        docID = docs_id_list.index(node[0])\n",
    "        freq = node[1]\n",
    "        m[i, docID] = (1 + math.log2(freq))*idf\n",
    "#     print(word, m[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a39c8709",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = np.sum(m**2, axis=0)\n",
    "norm = [math.sqrt(norm[i]) for i in range(len(norm))]\n",
    "# print(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29bc7b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_model(query):\n",
    "    query_words = prepare_words_from_text(query)\n",
    "    q_vector = np.zeros(len(unique_words_all))\n",
    "    for i in range(len(unique_words_all)):\n",
    "        word = unique_words_all[i]\n",
    "        if word in query_words:\n",
    "            ni = linked_list_data[word].n_docs\n",
    "            idf = math.log2(len(docs_id_list)/ni)\n",
    "            q_vector[i] = (1 + math.log2(query_words.count(word)))*idf\n",
    "    return q_vector\n",
    "    \n",
    "    \n",
    "q_vector = vector_model(queries['76']['title'])\n",
    "# print(q_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "983a3532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-4b9899f82ba4>:3: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ranking[j] = np.dot(m[:,j], q_vector) / norm[j]\n"
     ]
    }
   ],
   "source": [
    "ranking = np.zeros(len(docs_id_list))\n",
    "for j in range(len(docs_id_list)):\n",
    "    ranking[j] = np.dot(m[:,j], q_vector) / norm[j]\n",
    "#     print(docs_id_list[j], ranking[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683beb95",
   "metadata": {},
   "source": [
    "# Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ea90b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(filename):\n",
    "    file = open(filename, \"r\")\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce2a8093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': '76', 'file': '1040901_nation_story_3702283.utf8', 'relevant': '0'}\n"
     ]
    }
   ],
   "source": [
    "gt_filename = \"../FIRE2010/en.qrels.76-125.2010.txt\"\n",
    "gt_file = open_file(gt_filename)\n",
    "\n",
    "def parse_gt(line):\n",
    "    data = line.strip().split(' ')\n",
    "    return {\n",
    "        'index': data[0],\n",
    "        'file': data[2],\n",
    "        'relevant': data[3]\n",
    "    }\n",
    "\n",
    "with open(gt_filename) as fd:\n",
    "    content = fd.readlines()\n",
    "    print(parse_gt(content[0]))\n",
    "#     for line in content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5e469e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
