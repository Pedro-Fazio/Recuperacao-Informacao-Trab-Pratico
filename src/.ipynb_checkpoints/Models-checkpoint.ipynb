{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed647202",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rafael/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rafael/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import nltk\n",
    "import xmltodict\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2c65359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    regex = re.compile('[^a-zA-Z0-9\\s]')\n",
    "    text_returned = re.sub(regex, '', text)\n",
    "    return text_returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1942b921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_all_unique_words_and_freq(dict_global, words):\n",
    "    for word in set(words):\n",
    "        if word in dict_global.keys():\n",
    "            dict_global[word] += words.count(word)\n",
    "        else:\n",
    "            dict_global[word] = words.count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f004810",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stops = set(stopwords.words('english'))\n",
    "    \n",
    "def remove_stop_words(all_words):\n",
    "    words = []\n",
    "    for word in all_words: \n",
    "        if word not in en_stops:\n",
    "            words.append(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be8c55c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(words_index, global_dictionary, filename):\n",
    "    with open(filename) as fd:\n",
    "        doc = xmltodict.parse(fd.read())\n",
    "        text = doc['DOC']['TEXT']\n",
    "        if not isinstance(text, str):\n",
    "            print(filename)\n",
    "            return\n",
    "        text = remove_special_characters(text)\n",
    "        words = word_tokenize(text)\n",
    "        words = [word.lower() for word in words]\n",
    "        words = remove_stop_words(words)\n",
    "        finding_all_unique_words_and_freq(global_dictionary, words)\n",
    "        index = os.path.basename(filename)\n",
    "        words_index[index] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "797554b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    return list(set(lst1) & set(lst2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bc6b42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, docId, freq):\n",
    "        self.freq = freq\n",
    "        self.doc = docId\n",
    "        self.next = None\n",
    "    \n",
    "    def __str__(self):        \n",
    "        return 'doc:' + str(self.doc) + ', freq:' + str(self.freq)\n",
    "\n",
    "class LinkedList:\n",
    "    def __init__(self):\n",
    "        self.head = None\n",
    "        self.tail = None\n",
    "        self.n_docs = 0\n",
    "    \n",
    "    def print_list(self):\n",
    "        aux = self.head\n",
    "        while aux:\n",
    "            print(aux)\n",
    "            aux = aux.next\n",
    "    \n",
    "    def get_doclist(self):\n",
    "        l = []\n",
    "        aux = self.head\n",
    "        while aux:\n",
    "            l.append([aux.doc, aux.freq])\n",
    "            aux = aux.next\n",
    "        return l\n",
    "    \n",
    "    def add_doc(self, doc, freq):\n",
    "        node = Node(doc, freq)        \n",
    "        if self.head == None:\n",
    "            self.head = node        \n",
    "        else:\n",
    "            self.tail.next = node\n",
    "        self.tail = node\n",
    "        self.n_docs += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8949b30",
   "metadata": {},
   "source": [
    "# Terms Database\n",
    "\n",
    "A seção a seguir é responsável por construir o banco de dados por termos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bacf54ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../FIRE2010/en.doc.2010/TELEGRAPH_UTF8/2006_utf8/nation\n",
      "../FIRE2010/en.doc.2010/TELEGRAPH_UTF8/2006_utf8/nation/1060408_nation_story_6073156.utf8\n"
     ]
    }
   ],
   "source": [
    "# root = \"../FIRE2010/en.doc.2010/TELEGRAPH_UTF8\"\n",
    "root = \"../FIRE2010/en.doc.2010/TELEGRAPH_UTF8/2006_utf8/nation\"\n",
    "\n",
    "global_dictionary = {}\n",
    "words_index = {}\n",
    "processed_files = 0\n",
    "\n",
    "for path, subdirs, files in os.walk(root):\n",
    "    print(\"processing \" + path)\n",
    "    for name in files:\n",
    "        filename = os.path.join(path, name)\n",
    "        if filename.endswith(\".utf8\"):\n",
    "            processed_files += 1\n",
    "            process_file(words_index, global_dictionary, filename)\n",
    "            \n",
    "processed_files = len(words_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31070bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_id_list = [*words_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba67d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_all = sorted(set(global_dictionary.keys()))\n",
    "linked_list_data = {}\n",
    "\n",
    "for word in unique_words_all:\n",
    "    linked_list_data[word] = LinkedList()\n",
    "\n",
    "for doc in words_index.keys():\n",
    "    words = words_index[doc]\n",
    "    for word in set(words):\n",
    "        linked_list_data[word].add_doc(doc, words.count(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018415df",
   "metadata": {},
   "source": [
    "# Queries\n",
    "\n",
    "A seção a seguir é responsável por carregar o arquivo com as queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca242dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_filename = \"../FIRE2010/en.topics.76-125.2010.txt\"\n",
    "\n",
    "queries = {}\n",
    "\n",
    "with open(queries_filename) as fd:\n",
    "    doc = xmltodict.parse(fd.read())\n",
    "    for query in doc['topics']['top']: \n",
    "        queries[query['num']] = {\n",
    "            'title': query['title'],\n",
    "            'desc': query['desc'],\n",
    "            'narr': query['narr'],\n",
    "            '@lang': query['@lang']\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3ceee6",
   "metadata": {},
   "source": [
    "# Probabilistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22ecac5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def probabilistic(query):\n",
    "    query = remove_special_characters(query)\n",
    "    query = word_tokenize(query.lower())\n",
    "\n",
    "    answer = {}\n",
    "    for doc in words_index.keys():\n",
    "        words = words_index[doc]\n",
    "        common_words = intersection(query, words)\n",
    "        score = 0\n",
    "        for ki in common_words:\n",
    "            score += math.log10((processed_files+0.5)/(linked_list_data[ki].n_docs+0.5))\n",
    "        answer[doc] = score\n",
    "    return answer\n",
    "\n",
    "\n",
    "res = probabilistic(queries['76']['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec7113",
   "metadata": {},
   "source": [
    "# Vector Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a2a5639",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.zeros((len(unique_words_all), len(docs_id_list)))\n",
    "\n",
    "for i in range(len(unique_words_all)):\n",
    "    word = unique_words_all[i]\n",
    "    postings = linked_list_data[word].get_doclist()\n",
    "    ni = linked_list_data[word].n_docs\n",
    "    for node in postings:\n",
    "        docID = docs_id_list.index(node[0])\n",
    "        freq = node[1]\n",
    "        m[i, docID] = freq\n",
    "#     print(word, m[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3bc3cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.zeros((len(unique_words_all), len(docs_id_list)))\n",
    "for i in range(len(unique_words_all)):\n",
    "    word = unique_words_all[i]\n",
    "    postings = linked_list_data[word].get_doclist()\n",
    "    ni = linked_list_data[word].n_docs\n",
    "    idf = math.log2(processed_files/ni)\n",
    "    for node in postings:\n",
    "        docID = docs_id_list.index(node[0])\n",
    "        freq = node[1]\n",
    "        m[i, docID] = (1 + math.log2(freq))*idf\n",
    "#     print(word, m[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e414858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = np.sum(m**2, axis=0)\n",
    "norm = [math.sqrt(norm[i]) for i in range(len(norm))]\n",
    "# print(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c690edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_model(query):\n",
    "    query = remove_special_characters(query)\n",
    "    query = word_tokenize(query)\n",
    "    query = [q.lower() for q in query]\n",
    "    q_vector = np.zeros(len(unique_words_all))\n",
    "    for i in range(len(unique_words_all)):\n",
    "        word = unique_words_all[i]\n",
    "        if word in query:\n",
    "            ni = linked_list_data[word].n_docs\n",
    "            idf = math.log2(processed_files/ni)\n",
    "            q_vector[i] = (1 + math.log2(query.count(word)))*idf\n",
    "    return q_vector\n",
    "    \n",
    "    \n",
    "q_vector = vector_model(queries['76']['title'])\n",
    "# print(q_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65755389",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 6146 is out of bounds for axis 1 with size 6146",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6d44077ae845>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mranking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mranking\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_vector\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#     print(docs_id_list[j], ranking[j])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 6146 is out of bounds for axis 1 with size 6146"
     ]
    }
   ],
   "source": [
    "ranking = np.zeros(processed_files)\n",
    "for j in range(processed_files):\n",
    "    ranking[j] = np.dot(m[:,j], q_vector) / norm[j]\n",
    "#     print(docs_id_list[j], ranking[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0036ed40",
   "metadata": {},
   "source": [
    "# Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eed4fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(filename):\n",
    "    file = open(filename, \"r\")\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195ef86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_filename = \"../FIRE2010/en.qrels.76-125.2010.txt\"\n",
    "gt_file = open_file(gt_filename)\n",
    "\n",
    "def parse_gt(line):\n",
    "    data = line.strip().split(' ')\n",
    "    return {\n",
    "        'index': data[0],\n",
    "        'file': data[2],\n",
    "        'relevant': data[3]\n",
    "    }\n",
    "\n",
    "with open(gt_filename) as fd:\n",
    "    content = fd.readlines()\n",
    "    print(parse_gt(content[0]))\n",
    "#     for line in content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5e4eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
